{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd00decfbe5a70d8910007bbce8687584085ed75017321d767c169c5dae039dd627",
   "display_name": "Python 3.6.13 64-bit ('vgm-tf-keras-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Summary of Purpose\n",
    "In this demo, two MIDI (Musical Instrument Digital Interface) files will be created. One of those MIDIs is the result of a predictions from a recurrent autoencoder and the other will be the song that was used as input for the recurrent autoencoder. The two songs similarity will then be compared using Kullback-Liebler Divergence. The MIDIs should be viewed and played for qualitative results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# Neural Nets\n",
    "import tensorflow.keras as keras\n",
    "from keras.losses import KLDivergence\n",
    "\n",
    "# Generating Midis\n",
    "from music21 import *\n",
    "\n",
    "# Data Wrangling\n",
    "import numpy as np\n",
    "\n",
    "# Misc\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from IPython.display import display"
   ]
  },
  {
   "source": [
    "# Loading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pickled data loaded into program memory\n"
     ]
    }
   ],
   "source": [
    "# Load the onehot train/val/holdval split labeled dataset\n",
    "# Note: The file name indicates that the loaded data is `labeled` (i.e., multilabel onehot encoded chords)\n",
    "cwd = os.getcwd()\n",
    "with open(os.path.join(cwd, 'piano_note_encoding_dict'), 'rb') as fobj:\n",
    "    encoding_dict = pickle.load(fobj)\n",
    "\n",
    "with open(os.path.join(cwd, 'piano_holdval_array_for_time_series_LABELED_8_input_8_output'), 'rb') as fobj:\n",
    "    X_holdout_validation = pickle.load(fobj)\n",
    "\n",
    "# LOG\n",
    "print('Pickled data loaded into program memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_holdout_validation: (4620, 8, 98)\nA single record of X_holdout_validation is a 3D array with shape (1, 8, 98) where each dimension corresponds to a format of (ith_slice_of_a_song, t_timesteps, n_notes). `t_timesteps` is the number of notes in a sequence and the `n_notes` is the number of labels in the dataset.\n\nencoding_dict keys: ['int_to_str_chord', 'str_to_int_chord']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the loaded dictionary keys\n",
    "print('X_holdout_validation:', X_holdout_validation.shape)\n",
    "print('A single record of X_holdout_validation is a 3D array with shape (1, 8, 98) where each dimension corresponds to a format of (ith_slice_of_a_song, t_timesteps, n_notes). `t_timesteps` is the number of notes in a sequence and the `n_notes` is the number of labels in the dataset.')\n",
    "print()\n",
    "print('encoding_dict keys:', list(encoding_dict.keys()))"
   ]
  },
  {
   "source": [
    "## Description of the Data and the Contents of Each Unpickled Objects\n",
    "### `encoding_dict`\n",
    "Approximately 30 Final Fantasy MIDI files were processed, and the number of unique notes across *all* MIDIs was determined. Each string form of the note was mapped to an integer. For a very simple example, a single song might be composed of four notes/chords. Each chord is represented as a list of strings shown below:\n",
    "\n",
    "First Chord:  [[\"C4\", \"D4\", \"E4\"]]<br/>\n",
    "Second Chord: [[\"G3\"]]<br/>       \n",
    "Third Chord:  [[\"E-4\", \"B-4\"]]<br/>\n",
    "Fourth Chord: [[\"D4\", \"F4\", \"G#4\"]]<br/>\n",
    "\n",
    "Footnotes about these chords: <br/>\n",
    "1. Letter denotes which musical note and the number denotes which octave (basically the pitch).  <br/>\n",
    "2. A single letter, while technically a note and not a chord, is referred to as a chord. <br/>\n",
    "3. The '-' and the '#' symbols denote a note that is 'flat' or 'sharp,' respectively. <br/>\n",
    "\n",
    "Therefore, the integer mapping for these strings for this simple set of chords would consider only the number of unique notes (in this case there are eight unique notes and one repeated \"D4\"). The dictionary that maps each of these string notes to an integer would therefore be of length eight.\n",
    "\n",
    "### `X_holdout_validation`\n",
    "This consists of sliding window (slices) of length eight from each song from the NES Final Fantasy game. A single input record represents the eight notes/chords in sequence and the corresponding output record would be the next sequence of eight notes. Each note or chord is onehot encoded for multiple labels. This is because a chord is composed of several notes, and each note has a particular integer associated with it. Therefore, if the integers 0, 1, and 2 map to \"C4\", \"E4\", and \"G4\", the corresponding onehot vector for this chord is simply, <br/><br/>\n",
    "[[1 1 1 ....... 0]]. <br/>\n",
    "\n",
    "A one hot vector has a number of elements equal to the number of unique notes found across the whole dataset, and therefore can represent any chord in the entire dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Functions and Summary of Functions\n",
    "`make_predictions` will take a the recurrent autoencoder model and input to that model input and then produce a generated song where the number of notes/chords is determined by the model architecture (in this case output=8). <br/>\n",
    "`onehot_label_nn_output` will take an array of probabilities (which is the output of the neural network) and 'hot' a particular label if the label's value exceeds the minimum positive classification threshold. <br/>\n",
    "`make_music21_stream` converts an array of onehot vectors into a MIDI writable format."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X_test, positive_classification_threshold):\n",
    "    \"\"\"Builds two onehot matrices (generated song and original song) AND probability array for generated song.\n",
    "\n",
    "    :param model: keras model\n",
    "    :param X_test: <class 'numpy.ndarray'> from which a random chord\n",
    "        sequence will be selected.\n",
    "    :param positive_classification_threshold: <class 'float'> that\n",
    "        determines the minimum probability that an output\n",
    "        neuron must have in order to be considered a positive classification\n",
    "        for a particular category.\n",
    "    :return: <class 'tuple'> of <class 'numpy.ndarray'>\n",
    "    \"\"\"\n",
    "    # LOG\n",
    "    print('Generating music')\n",
    "\n",
    "    # Occurrence of failed threshold\n",
    "    cnt_failed_threshold = 0\n",
    "\n",
    "    # Take random starting starting point for validation\n",
    "    random_ix_of_sequence_elem_in_x_test = np.random.randint(\n",
    "        0, X_test.shape[0])\n",
    "\n",
    "    ## Variables to be modified in song generation loop\n",
    "\n",
    "    # The one hot generated song\n",
    "    generated_song = np.empty(shape=(1, 0, X_test.shape[2]))\n",
    "\n",
    "    # The generated song as an array of probabilities\n",
    "    generated_song_probability_array = np.empty(shape=(1, 0, X_test.shape[2]))\n",
    "\n",
    "    # The input to the model\n",
    "    input_tensor = X_test[random_ix_of_sequence_elem_in_x_test].reshape(\n",
    "        1, X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "    # A copy of the original input\n",
    "    original_song = input_tensor.copy()\n",
    "\n",
    "    # (?, output_timestep, categories)\n",
    "    predicted_chords_tensor = model.predict(\n",
    "        input_tensor, verbose=0)\n",
    "\n",
    "    # A sample is (timesteps, categories) dimensional\n",
    "    for sample in predicted_chords_tensor:\n",
    "\n",
    "        # A chord is (labels, ) dimensional\n",
    "        for ix, chord_ in enumerate(sample):\n",
    "\n",
    "            # Append to the probability array\n",
    "            generated_song_probability_array = np.append(\n",
    "                generated_song_probability_array, \n",
    "                chord_.reshape(1, 1, predicted_chords_tensor.shape[2]),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Convert the array of probabilities to a one hot vector\n",
    "            # representing that chord\n",
    "            chord_ = onehot_label_nn_output(\n",
    "                chord_, positive_classification_threshold)\n",
    "\n",
    "            # Append the chord to the generated song but if no chord\n",
    "            # is generated, just take the previous chord from the input sequence\n",
    "            chord_ = chord_.reshape(1, 1, predicted_chords_tensor.shape[2])\n",
    "            if (np.amax(chord_) == 0):\n",
    "\n",
    "                # If no classification meets the\n",
    "                # positive_classification_threshold, then the one-hot\n",
    "                # vector will be all 0s. Therefore, the output\n",
    "                # will be estimated as the very last element in the\n",
    "                # input sequence. Since the prediction_input_matrix\n",
    "                # has dims (?, timestep, categories) then [-1][-1]\n",
    "                # gets the last time step's chord represented by\n",
    "                # a vector (categories,)\n",
    "\n",
    "                # Map input directly to output\n",
    "                filler_chord_from_input_tensor = input_tensor[0][ix].reshape(\n",
    "                    1, 1, input_tensor.shape[2])\n",
    "                generated_song = np.append(\n",
    "                    generated_song, filler_chord_from_input_tensor, axis=1)\n",
    "\n",
    "                # Incremement the number of failed classifications\n",
    "                cnt_failed_threshold += 1\n",
    "            else:\n",
    "                generated_song = np.append(\n",
    "                    generated_song, chord_, axis=1)\n",
    "\n",
    "    # Return the onehot multilabeled generated song\n",
    "    # and the original song used to generate it\n",
    "    print('Music generated.')\n",
    "    print('Number of failed predictions for the generated song:', cnt_failed_threshold)\n",
    "    return (generated_song, original_song, generated_song_probability_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_label_nn_output(chord, positive_classification_threshold):\n",
    "    \"\"\"Takes a chord vector output from nn and converts to onehot vector.\n",
    "    \n",
    "    Get the element of a chord (which is probability vector)\n",
    "    and hot a category based on probability threshold.\n",
    "\n",
    "    :param chord: <class 'numpy.ndarray'> of probabilities\n",
    "        for multilabel classification\n",
    "    :param positive_classification_threshold: <class 'float'> minimum\n",
    "        threshold that an element of the probability vector must\n",
    "        exceed in order to decide positive classification of a label (note)\n",
    "        or not.\n",
    "    :return: <class 'numpy.ndarray'> one hot vector\n",
    "    \"\"\"\n",
    "    # Iterate through labels in chord\n",
    "    for ix, label in enumerate(chord):\n",
    "        if (label > positive_classification_threshold):\n",
    "            chord[ix] = 1\n",
    "        else:\n",
    "            chord[ix] = 0\n",
    "\n",
    "    # Return the labeled chord\n",
    "    return chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_music21_stream(onehot_matrix, int_to_str_chord, instrument_part=None):\n",
    "    \"\"\"Converts one-hot matrices to writable songs.\n",
    "\n",
    "    :param onehot_matrix: <class 'numpy.ndarray'> of shape\n",
    "        (? =~ 1, timestep, classes) to be converted to string chords.\n",
    "    :param int_to_str_chord: <class 'dict'> that maps integers to\n",
    "        individual notes (still referred to as chords).\n",
    "    :param instrument_part: <class 'music21.stream.instrument.Instrument'>\n",
    "        to be used for the stream generation.\n",
    "    \"\"\"\n",
    "    # Default instrument\n",
    "    if (not instrument_part):\n",
    "        instrument_part = instrument.KeyboardInstrument()\n",
    "\n",
    "    # The music stream\n",
    "    music21_stream = stream.Part()\n",
    "    music21_stream.append(instrument_part)\n",
    "\n",
    "    # Iterate through songs (should just be 1 song)\n",
    "    for song in onehot_matrix:\n",
    "\n",
    "        # A song will have some number of chords determined a priori\n",
    "        for chord_ in song:\n",
    "\n",
    "            # A string'ified musical element representing\n",
    "            # a note or a chord\n",
    "            musical_element = []\n",
    "            for ix, label in enumerate(chord_):\n",
    "\n",
    "                # Get the predicted musical element as a list of strings\n",
    "                if (label == 1):\n",
    "                    musical_element.append(int_to_str_chord[ix])\n",
    "\n",
    "            # If the length of the musical element is 1 then the musical element\n",
    "            # must be a NOTE otherwise it's a collection of NOTES aka a CHORD\n",
    "            if (len(musical_element) == 1):\n",
    "                music21_stream.append(note.Note(musical_element[0]))\n",
    "            else:\n",
    "                music21_stream.append(chord.Chord(musical_element))\n",
    "\n",
    "    # Return the musical score\n",
    "    return music21_stream"
   ]
  },
  {
   "source": [
    "# Testing the Recurrent Autoencoder Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recurrent autoencoder model\n",
    "model = keras.models.load_model(os.path.join(cwd, '20210502_08-25-53_max_kld_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating music\n",
      "Music generated.\n",
      "Number of failed predictions for the generated song: 2\n"
     ]
    }
   ],
   "source": [
    "# Make predicted onehot labeled songs\n",
    "positive_classification_threshold = 0.2\n",
    "(onehot_generated_song, onehot_original_song, generated_song_proba_arr) = make_predictions(\n",
    "    model,\n",
    "    X_holdout_validation,\n",
    "    positive_classification_threshold=positive_classification_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original and generated song converted to music21 objects.\n"
     ]
    }
   ],
   "source": [
    "## Convert the output to music21 objects\n",
    "\n",
    "# The song generated by the model\n",
    "generated_song = make_music21_stream(\n",
    "    onehot_generated_song,\n",
    "    int_to_str_chord= encoding_dict['int_to_str_chord']\n",
    ")\n",
    "\n",
    "# The slice of the song from the Final Fantasy NES Game (aka the `original` or `template` song)\n",
    "original_song = make_music21_stream(\n",
    "    onehot_original_song,\n",
    "    int_to_str_chord=encoding_dict['int_to_str_chord']\n",
    ")\n",
    "\n",
    "# LOG\n",
    "print('Original and generated song converted to music21 objects.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Midis successfully written.\n"
     ]
    }
   ],
   "source": [
    "## Write the songs as midi files to file\n",
    "\n",
    "# Create a destination directory\n",
    "if not os.path.exists(os.path.join(cwd, 'midis')):\n",
    "    os.mkdir(os.path.join(cwd, 'midis'))\n",
    "\n",
    "# Write the songs\n",
    "now = datetime.datetime.now().strftime('%Y%m%d_%H-%M-%S')  # Current date and time\n",
    "generated_song.write('midi', os.path.join(cwd, 'midis', f'{now}_generated_song.mid'))\n",
    "original_song.write('midi', os.path.join(cwd, 'midis', f'{now}_original_song.mid'))\n",
    "\n",
    "# LOG\n",
    "print('Midis successfully written.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "When comparing the same song, the KL Divergence: 0.0\nWhen comparing the generated song and the original song, the KL Divergence: 13.34582\n"
     ]
    }
   ],
   "source": [
    "## Compare similarity\n",
    "\n",
    "# Kullback-Leibler Divergence compares the `statistical similarity` between distributions\n",
    "kl = KLDivergence()\n",
    "kl_original = kl(\n",
    "    onehot_original_song.astype('float32').reshape(X_holdout_validation.shape[1], X_holdout_validation.shape[2]), \n",
    "    onehot_original_song.astype('float32').reshape(X_holdout_validation.shape[1], X_holdout_validation.shape[2])\n",
    ")\n",
    "\n",
    "kl_generated = kl(\n",
    "    onehot_original_song.astype('float32').reshape(X_holdout_validation.shape[1], X_holdout_validation.shape[2]), \n",
    "    generated_song_proba_arr.astype('float32').reshape(X_holdout_validation.shape[1], X_holdout_validation.shape[2])\n",
    ")\n",
    "\n",
    "# LOG\n",
    "print('When comparing the same song, the KL Divergence:', kl_original.numpy())\n",
    "print('When comparing the generated song and the original song, the KL Divergence:', kl_generated.numpy())"
   ]
  },
  {
   "source": [
    "# Interpreting the Results\n",
    "For music, ultimately the best thing to do is to listen to the music produced. However, the KL Divergence is a metric that is commonly used for multilabel classification tasks as it is a measure of how different two probability distributions are. Since the output of the neural network is an array of vectors of probabilties, and the onehot vector that the result is based on can be interpreted as an array of probabilities (e.g., [[0 0 0 1]] is a onehot vector but the probability that a label is the 4th label is simply 100%), then the KL divergence can be used to compare these two probability distributions. If the KL Divergence is 0, the probability distributions of the generated music and the original music is exactly equal. Otherwise, as the KL Divergence increases, so too does the quantitative dissimilarity between the two pieces."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# References\n",
    "## Frequently Used Tutorials\n",
    "* Keras Blog\n",
    "    * https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "* LSTM Auto-Encoder\n",
    "    * https://machinelearningmastery.com/lstm-autoencoders/\n",
    "* TF tutorial\n",
    "    * https://www.datacamp.com/community/tutorials/using-tensorflow-to-compose-music\n",
    "    \n",
    "## Academic Papers and Other Materials\n",
    "[[1]] M. Newman, \"Video Game Music Archive: Nintendo Music,\" VGMusic, 1996. [Online]. Available: https://www.vgmusic.com/music/console/nintendo/nes/. [Accessed 08 March 2021]. <br/>\n",
    "[[2]] S. AlSaigal, S. Aljanhi and N. Hewahi, \"Generation of Music Pieces Using Machine Learning: Long Short-Term Memory Neural Networks Approach,\" Arab Journal of Basic and Applied Sciences, vol. 26, no. 1, pp. 397-413, 2019. <br/>\n",
    "[[3]] N. Mauthes, VGM-RNN: Recurrent Neural Networks for Video Game Music Generation Generation, Master's Projects, 2018, p. 595. <br/>\n",
    "[[4]] A. Geron, \"Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANS,\" in Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd ed., Sebastopol, O'Reilly Media, Inc, 2019, pp. 567-574. <br/>\n",
    "[[5]] J. Briot, G. Hadjeres and F. Pachet, \"Deep Learning Techniques for Music Generation - A Survey,\" arXiv:1709.01620, 2017. <br/>\n",
    "[[6]] A. Geron, \"Chapter 15: Processing Sequences Using RNNs and CNNs,\" in Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd ed., Sebastopol, O'Reilly Media, Inc., 2019, pp. 497-499. <br/>\n",
    "[[7]] S. Hochreiter, \"The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions,\" International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 6, no. 2, pp. 107-116, 1998. <br/>\n",
    "[[8]] J. Svegliato and S. Witty, \"Deep Jammer: A Music Generation Model,\" University of Massachusetts, Amherst, 2016.\n",
    "D. Kang, J. Kim and S. Ringdahl, \"Project milestone: Generating music with Machine Learning,\" Stanford University, Stanford, 2018. <br/>\n",
    "[[9]] A. Ycart and E. Benetos, \"A Study on LSTM Networks for Polyphonic Music Sequence Modelling,\" in 18th International Society for Music Information Retrieval Conference (ISMIR), Suzhou, 2017. <br/> \n",
    "[[10]] A. Huang and R. Wu, \"Deep Learning for Music,\" Stanford University, Stanford. <br/>\n",
    "[[11]] L. Yang and A. Lerch, \"On the Evaluation of Generative Models in Music,\" Neural Computing and Application, vol. 32, no. 9, p. 12, 2018.<br/> \n",
    "[[12]] A. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior and K. Kavukcuoglu, \"Wavenet: A Generative Model for Raw Audio,\" arXiv:1609.03499, 2016. <br/>\n",
    "[[13]] J. Ba, J. Kiros and G. Hinton, \"Layer Normalization,\" arXiv:1607.06450 , 2016. <br/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}